JOBS
------------
- Jobs are tasks with a scheduled time to run or repeat
- A scheduler object will trigger jobs from itself
- 


ARGUMENTS
------------

Task objects have 1..* relation to Arguments which hold information about the type and name of arguments to the task. If a Plug is attached directly to an Argument then the receiving Socket will wait on all Argument values before invoking the task. However, if a Plug is attached directly to a Socket, then the Socket will dispatch the request and parameters as-is to the function.


WEBSOCKETS
------------

- Individual web socket containers (streamers) listen to redis pubsub channels (e.g. proc1.task) and
forward messages to a websocket it hosts. any websocket client can connect and receive messages
- This keeps the execution network de-coupled from the notification/subscribe network
- A grafana datasource can connect directly to the websocket and render incoming data


- one docker service per redis<->websocket bridge
   - creates websocket server and subscribes to redis channel
- websocket client<->influxdb,elastic, etc
   - create client adaptors for anything
   - data is ephemeral
- grafana/kibana real time charts

SECURITY
------------

- Client API and classes pull login from pyfi.ini file. 
- User has to "pyfi login" successfully to get a valid token
- Token is compared to user object "token" field.

- pyfi.polar defines the object model security policies
- get_checked_permissions is generated from queries on privileges applied to model objects
- 



- Use the processor client API to write complex, parallel workflows on top of processors

----

Pyfi provides a set of interacting compute layers that control the location and execution of managed code assets.
With PYFI, code modules and functions can be loaded in multiple locations and invoked from clients without knowledge of where or how those functions are called.
Redundant code (processors) loaded into a PYFI network will be able to respond to higher volume of data and requests and thus can scale at will.

Functional tasks (processors hosting code) are fronted by durable queues that deliver reliable invocations when those functions are present on the network, regardless of their exact location.
This allows the system to be resilient to hardware or network changes, as well as influence by schedulers that might change the location of functions (processors) to re-balance the resources across the network.

All of this underlying management, hardware arriving and departing, services starting and stopping, processors moving from one host to another (or failing), is completely invisibile to the applications and clients using the system.
To them, function calls will always, eventually be executed, if not immediately, in the near future when compute resources allow it.

----

Agent spawns sub processes that invoke `pyfi worker start -n workername` inside a virtualenv where the gitrepo code is loaded and installed.
Thus, the worker runs inside it's own virtualenv. If the gitrepo is updated, the agent can restart the processor which will load a new environment

Processor inbound queues map directly to sockets->task names. Thus, each socket+function have their own queues. 
A top level queue for the processor is linked to a fanout exchange where all processors connected to the same base queue exchange will receive same message (broadcast).

- Run pyfi worker start brings up celery worker with correct queues. DONE
- Sending message using API invokes new worker DONE
- Linked processors execute code NOT TESTED WITH NEW DESIGN YET
- 

--------------------------

SCHEDULER->NODE->AGENT->WORKER->PROCESSOR->SOCKET->TASK->CALL

PYFI - CLI for pyfi admin and monitoring - queries celery/amqp network along with pyfi database

PROCESSORS - Each python processor script is wrapped in a pyfi harness that marshalls messages and events from amqp to the script
and provides the script with additional framework support routines. A single processor in a flow can be run across many servers and workers (CPUs)
giving it scalability, high-availability and redundancy that is separate from the behavior of other processors or nodes.

RABBITMQ - Provides the persistent transactional messaging layer

ACTION - And action for an agent to perform

CELERY - Distributed task service on top of RabbitMQ. Processors trigger invocations to other processors via unique task queues.
This allows for HA and redundancy as celery workers can be spawned for each processor listening on respective queues.

FLOW - Represents the messaging queue topologies and tasks (processors). Flows are designed and then built because specific workers
need to be spawned across targeted nodes that run the processor and listen on inbound queues. Processor harness code tracks and monitors
state in the pyfi database so the UI can monitor what processors are built and running, etc.

OBJECTS - A flow contains various kinds of objects on the canvas. Script objects are python scripts that process inbound data
and send results to outbound ports for other processors to receive. Logic objects are special kinds of script objects that
perform certain flow logic like conditional routing (or router). A router will evaluate all the inbound data and send it to
one or more outbound queues depending on some internal logic it implements. Best practive is for the router to not manipulate
the outbound data.

SERVER - The PYFI server orchestrates the remote celery workers by creating ssh processes that are connected to them. Anytime 
processor changes, pyfi server can restart all its workers using the new processor task and schedule (e.g. how often it runs)
The server process also maintains the pyfi database and registry. When the server spawns a remote worker task, information
is embedded in the task about the pyfi cluster such as where the server API is located. This allows remote workers to reconnect
to the server should the server go down.

Processors will retrieve their state information from the server in a stateless manner. If that state indicates the processor
should terminate, then it terminates. If a script update is available in the database, the worker will request a refresh and then
that process will terminate after the new process has started - when the server API returns.

SERVICES design
---------------

Using supervisord pyfi agent start and pyfi agent start --queues are run as process apps on each hosts
This ensures that the host is available for processors to run on
From there, the agent will monitor the datbase for new processors targeted for that host
A separate scheduler will pair processor records with agents based on available resources.
The scheduler may rebalance the network by re-assigning processors that are packed onto a single server.
For example, if server A has 12 CPUS and 4 processors with concurrency of 3 and server B has 12 CPUs with no processors.
The scheduler will move 2 of the processors to server B. If later, a new processor requests a concurrency of 12,
the scheduler can move the 2 processors on server B, back to server A and then assign the new 12 CPU processor to server B.


FILESYSTEM design
-----------------

A special pyfi fuse filesystem reflects the contents of the database, allowing 3rd party commands and apps to work seamlessly with
the pyfi cluster. Since the database is the single-source-of-truth only changes made through it are reflected in the network.

For example, to list all the processors in pyfi using the filesystem, you simply:

$ cd pyfi
$ ls processors/

If you want to add a new processor, you can create a new processor document in the processors directory and the filesystem will
commit it to the database or report an error

If you want to add a new server to the network, simply do this:

$ cd servers
$ mkdir my.server.name
$ cd my.server.name
$ mkdir processors
$ cd processors
$ vi proc1.json
$ cd ../agents
$ ls 
$ cd ..
$ cd modules
$ git clone http://my.git.repo with python code (the code is then copied to the host and available to the agent)

When you make a new server directory in servers the filesystem will deploy pyfi to that host using ssh.
Creating or copying packages within those server directories makes them available to agents running on that server.

CLI design
----------

CLI commands add an ACTION record to the database and the PYFI agent implements them across the network.
Actions can be targeted to specific hosts or workers.

AGENT design
------------

Agent process monitors processor table and launchers workers to serve processor needs (e.g. concurrency)
Agent will remember which processors it is responsible for and restart them when it boots
Processor lifecycle is managed through database using its requested_status attribute. Agent then responds
When the processor achieves its state, agent changes requested_status to 'ready' and status to the desired state.
If any error occurs it is stored in the error field (TBD).

Any queues associated with the processor are mapped down to the worker and celery worker.
Whenever the processor state changes, the agent will kill the current worker and create a new one


DEPLOY 
------

- Processors are added to the database
- Aftewards, agents will obtain listed processors (read lock) and create a worker with the concurrency needs of the processor, 
passing in the module/code for the processor and assigning the worker to the queues of the processor.
   - Whenever objects in the flow change such as processor concurrency, or connected queues, it is tagged with an "update" status
   and the agent will terminate existing workers assigned to the processor and respawn new ones.