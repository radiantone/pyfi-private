
PYFI - CLI for pyfi admin and monitoring - queries celery/amqp network along with pyfi database

PROCESSORS - Each python processor script is wrapped in a pyfi harness that marshalls messages and events from amqp to the script
and provides the script with additional framework support routines. A single processor in a flow can be run across many servers and workers (CPUs)
giving it scalability, high-availability and redundancy that is separate from the behavior of other processors or nodes.

RABBITMQ - Provides the persistent transactional messaging layer

ACTION - And action for an agent to perform

CELERY - Distributed task service on top of RabbitMQ. Processors trigger invocations to other processors via unique task queues.
This allows for HA and redundancy as celery workers can be spawned for each processor listening on respective queues.

FLOW - Represents the messaging queue topologies and tasks (processors). Flows are designed and then built because specific workers
need to be spawned across targeted nodes that run the processor and listen on inbound queues. Processor harness code tracks and monitors
state in the pyfi database so the UI can monitor what processors are built and running, etc.

OBJECTS - A flow contains various kinds of objects on the canvas. Script objects are python scripts that process inbound data
and send results to outbound ports for other processors to receive. Logic objects are special kinds of script objects that
perform certain flow logic like conditional routing (or router). A router will evaluate all the inbound data and send it to
one or more outbound queues depending on some internal logic it implements. Best practive is for the router to not manipulate
the outbound data.

SERVER - The PYFI server orchestrates the remote celery workers by creating ssh processes that are connected to them. Anytime 
processor changes, pyfi server can restart all its workers using the new processor task and schedule (e.g. how often it runs)
The server process also maintains the pyfi database and registry. When the server spawns a remote worker task, information
is embedded in the task about the pyfi cluster such as where the server API is located. This allows remote workers to reconnect
to the server should the server go down.

Processors will retrieve their state information from the server in a stateless manner. If that state indicates the processor
should terminate, then it terminates. If a script update is available in the database, the worker will request a refresh and then
that process will terminate after the new process has started - when the server API returns.

SERVICES design
---------------

Using supervisord pyfi agent start and pyfi agent start --queues are run as process apps on each hosts
This ensures that the host is available for processors to run on
From there, the agent will monitor the datbase for new processors targeted for that host
A separate scheduler will pair processor records with agents based on available resources.
The scheduler may rebalance the network by re-assigning processors that are packed onto a single server.
For example, if server A has 12 CPUS and 4 processors with concurrency of 3 and server B has 12 CPUs with no processors.
The scheduler will move 2 of the processors to server B. If later, a new processor requests a concurrency of 12,
the scheduler can move the 2 processors on server B, back to server A and then assign the new 12 CPU processor to server B.


FILESYSTEM design
-----------------

A special pyfi fuse filesystem reflects the contents of the database, allowing 3rd party commands and apps to work seamlessly with
the pyfi cluster. Since the database is the single-source-of-truth only changes made through it are reflected in the network.

For example, to list all the processors in pyfi using the filesystem, you simply:

$ cd pyfi
$ ls processors/

If you want to add a new processor, you can create a new processor document in the processors directory and the filesystem will
commit it to the database or report an error

If you want to add a new server to the network, simply do this:

$ cd servers
$ mkdir my.server.name
$ cd my.server.name
$ mkdir processors
$ cd processors
$ vi proc1.json
$ cd ../agents
$ ls 
$ cd ..
$ cd modules
$ git clone http://my.git.repo with python code (the code is then copied to the host and available to the agent)

When you make a new server directory in servers the filesystem will deploy pyfi to that host using ssh.
Creating or copying packages within those server directories makes them available to agents running on that server.

CLI design
----------

CLI commands add an ACTION record to the database and the PYFI agent implements them across the network.
Actions can be targeted to specific hosts or workers.

AGENT design
------------

Agent process monitors processor table and launchers workers to serve processor needs (e.g. concurrency)
Agent will remember which processors it is responsible for and restart them when it boots
Processor lifecycle is managed through database using its requested_status attribute. Agent then responds
When the processor achieves its state, agent changes requested_status to 'ready' and status to the desired state.
If any error occurs it is stored in the error field (TBD).

Any queues associated with the processor are mapped down to the worker and celery worker.
Whenever the processor state changes, the agent will kill the current worker and create a new one


DEPLOY 
------

- Processors are added to the database
- Aftewards, agents will obtain listed processors (read lock) and create a worker with the concurrency needs of the processor, 
passing in the module/code for the processor and assigning the worker to the queues of the processor.
   - Whenever objects in the flow change such as processor concurrency, or connected queues, it is tagged with an "update" status
   and the agent will terminate existing workers assigned to the processor and respawn new ones.