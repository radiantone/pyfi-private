
Phase 1 Goals
-------------

- General UI design, initial implementation and flow DONE
- pyfi cli design DONE
- pyfi agent that monitors remote db and updates worker processes DONE
- pyfi api that executes tasks over pyfi cli workers controlled by pyfi cli agent only DONE (concept)
- execute dynamic code task over pyfi agent/worker network DONE(concept)
- pyfi cli database admin DONE(concept)
- pyfi database models DONE
- pyfi architecture slides DONE
- pyfi architecture containers (running)
- pyfi-ui repo DONE
- Data model design DONE


Phase 2 Goals
-------------

- Create/Manage pyfi network across machines DONE
- Remote deploy/manage of pyfi agents using paramiko ssh
- Remote syncing of venv's with pyfi
- Remotely execute tasks and dynamic code DONE
- pyfi cli network/queue/cluster/agent status DONE
- use postgres container for sqlalchemy DONE
- Detailed data model DONE
- data model/table migration DONE
- Use Cases:
    - Agents spawn workers for processors across machines  DONE
    - pyfi cli creates, starts, stops, restarts processors DONE
    - Agents resume workers after restart unless processor is in stopped model DONE
    - User modules are retrieved from git DONE
- --gitrepo, --commit and --dir options for agents to locate git repo and where to store it. Agent
then updates its local repo based on the desired commit level for the code before running the worker
so the proper code is in place. DONE
- Throughput TESTS DONE
- Fix memory issues DONE

Phase 3 Goals
-------------

- Monitor tasks using flower and rabbitmq admin. See individual task info. DONE
- Expand processor/task configs e.g. retries, etc.  DONE
- Implement logging inside task signal functions DONE
- Use flower API to get metadata?
- Stop a processor after restarting it 5 times in 5 minutes
- Test/Observe rate limiting
- Test/Observe worker scaling (via flower) and worker auto-scaling
- Initial PYFI API for UI
- Add pyfi API to compose file
- Add nginx to compose file to front PYFI API and Flower API into one API Route 65%
- Create nginx volume with app static build in it or mounted as volume DONE
- Run agent in supervisor, test fault tolerance and message reliability DONE

- Flow objects and data model
   - Link to all processors
   - Stopping or starting a flow invokes same method on processors

- Password protect processors: lock & unlock. Login dialogs

- Obtain queue message status from Flower for a particular queue or task 

- Add user CLI commands DONE
- Delete CLI commands DONE


Phase 4 Goals
-------------

- Build flows with CLI or client API DONE
- Invocations complete regardless of when processors come online DONE
    - Flows complete even if partially available DONE
- Queues are reclaimed after expiry when no consumers attached DONE
- APScheduler 100%
    - Manage task schedule from cli and client API
- ls calls, ls call->shows result DONE
    - Add result size to CallModel
- Port pubsub to redis DONE
- CPU affinity for worker processors, scheduler decides. UNK
- cli listen works with redis now DONE
- User RLS row level security in database DONE
- Scheduler CLI commands 80%
- Scheduler task
    - Monitors nodes associated with scheduler 40%
    - If a processor has no worker, find a node and hostname to the processor. Agent will take over then 20%
- ls node tree printer DONE
- More complete relational model 95%
- isolated virtualenvs for processors using github w/logins DONE
- Proper room joining DONE
- Proper queue subscriptions in worker DONE
- tasks can send output to 'log','task','data' channels 80%
- Move processor between hosts while running DONE
- Client API 80%
- Call graph CLI for calls DONE
- View results with CLI DONE


Phase 5 Goals
----------------------
+ Refactor data model to make more logical sense DONE
+ Update worker behavior DONE
+ Control flow mechanisms

- Scheduler managing nodes and processors. Attaching orphans, rebalancing etc 50%
- Pub sub working smoothly on all 3 processor channels: data, log, task DONE
     - Task lifecycle notifications
- Finish implementing worker signals 75%
- All control flow structures working DONE
- Performance bugs fixed DONE
- Remapped plugs, queues and sockets DONE
    - Plugs no longer point to processors. Only sockets. DONE
    - Sockets have a source and target plug DONE
    - Outbound data flow goes through plug queue then arrives on task queue DONE
    - Task results flow onto sourceplugs attached to socket. DONE
    - Add queue binding to plugs DONE
- Worker updates
    - Add all outbound data flows to sourceplugs and target sockets into a single parallel operation DONE
- RTD Docs. 80%


Phase 6 Goals
-----------------------
- UI templates for control flow processors
- Flask UI backend with login, cors and other harness plugins working
- Flask API REST server 
- 
